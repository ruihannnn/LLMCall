# ===============================================================================
# 配置说明：
# - 未注释的变量必须手动配置
# - 注释掉的变量是可选配置，默认值为注释中的值
# - 支持多个prompt模板同时处理，用逗号分隔
# - 当使用单个 PROMPT_KEY 时，支持多个 OUTPUT_COLUMN 和 RESPONSE_PROCESSOR，此时 OUTPUT_PROMPT_COLUMN 必须为 None 或者为 1 个，且 RESPONSE_PROCESSOR 数量必须与 OUTPUT_COLUMN 数量相同
# - 当使用多个 PROMPT_KEY 时，PROMPT_KEY 数量必须与 OUTPUT_COLUMN 数量一致；如果设置了 OUTPUT_PROMPT_COLUMN，其数量也必须与 PROMPT_KEY 数量一致；RESPONSE_PROCESSOR 数量必须为 1 个或与 PROMPT_KEY 数量相同
# - 新增分组模式：当使用多个 PROMPT_KEY 时，RESPONSE_PROCESSOR 和 OUTPUT_COLUMN 支持 [a,b],[c],[d,e] 格式，每个 [] 内的处理器和输出列数量必须相同
# ===============================================================================

# ==============日志配置==============
# 日志文件保存路径
LOG_FILE=log/log.txt

# 日志级别：DEBUG, INFO, WARNING, ERROR
# LOG_LEVEL=INFO

# ==============LLM服务配置==============
# LLM服务的API地址
LLM_URL=https://api.deepseek.com

# LLM服务的API密钥（本地部署可以不设置）
API_KEY=sk-4997df801a504725b8307770ea68dbb5

# LLM模型名称
MODEL_NAME=deepseek-chat

# 生成温度，控制输出随机性（0-1，越高越随机）
# TEMPERATURE=0.6

# 核采样参数，控制词汇选择范围（0-1）
# TOP_P=0.95

# 最大生成token数
# MAX_TOKENS=100000

# 停止词列表，JSON格式
# STOP=["<|endoftext|>"]

# ==============数据集配置==============
# 输入JSONL文件的完整路径
INPUT_PATH=C:\\开源代码\\LLMCall\\data\\input.jsonl

# 输出JSONL文件的完整路径（当输入输出文件相同时，不能设置 MAX_ROWS 限制，因为这会导致原文件中未处理的数据丢失。请使用不同的输出文件路径，或者将 MAX_ROWS 设置为 None）
OUTPUT_PATH=C:\\开源代码\\LLMCall\\data\\output.jsonl

# 从输入数据中提取哪些列作为输入，用逗号分隔
# 示例：session,query 表示使用 session 和 query 两列的数据
INPUT_COLUMNS=session,query

# 生成结果保存到哪一列
# 单个 PROMPT_KEY 时：支持多个输出列，可将多种后处理结果分别输出到不同的列中
# 多个 PROMPT_KEY 时：每个 prompt 的结果对应保存到相应位置的列，用逗号分隔
# 分组模式：支持 [col1,col2],[col3],[col4,col5] 格式，每个 [] 对应一个 PROMPT_KEY
# 示例：answer1,answer2 表示第 1 个 prompt 结果存到 answer1，第 2 个 prompt 结果存到 answer2
# 分组示例：[answer1,answer2],[answer3],[answer4,answer5] 表示第 1 个 prompt 的结果经过多个处理器后分别存到 answer1 和 answer2
OUTPUT_COLUMN=[answer_1_1,answer1_2],[answer2_1]

# 是否保存填充后的 prompt 文本，值为空时则不保存 prompt
# 单个 PROMPT_KEY 时：OUTPUT_PROMPT_COLUMN 必须为 None 或者为 1 个，所有输出列共享同一个 prompt
# 多个 PROMPT_KEY 时：每个 prompt 对应一个输出列，用逗号分隔
# 示例：prompt1,prompt2 表示第 1 个 prompt 文本存到 prompt1 列，第 2 个存到 prompt2 列
OUTPUT_PROMPT_COLUMN=prompt

# 批处理大小，影响内存使用和处理速度
# BATCH_SIZE=1000

# 限制处理的数据行数，用于测试（值为空时，处理全部数据）
# 注意：当输入输出文件相同时，不能设置此参数
# MAX_ROWS=5

# 最大并发线程数，影响处理速度
# MAX_THREAD_NUM=512

# ==============Prompt和响应处理配置==============
# Prompt模板名称，对应prompt.py文件中all_prompt_dict的键名
# 单个prompt：PROMPT_KEY=generate_assistant_new
# 多个prompt：PROMPT_KEY=generate_assistant_new,another_prompt_key （用逗号分隔）
# 注意：多个prompt时数量必须与OUTPUT_COLUMN数量一致（非分组模式）
PROMPT_KEY=generate_assistant_new,tmp

# 响应后处理函数名称，对应response_processor.py文件中的函数名
# 单个 PROMPT_KEY 时：RESPONSE_PROCESSOR 数量必须与 OUTPUT_COLUMN 数量相同，对同一个响应进行多种后处理
# 单个处理器（多个 PROMPT_KEY 时）：对所有 prompt 响应使用相同的后处理函数
#   示例：RESPONSE_PROCESSOR=no_think_response_processor
# 多个处理器（多个 PROMPT_KEY 时）：按顺序为每个 prompt 响应指定不同的后处理函数（用逗号分隔）
#   示例：RESPONSE_PROCESSOR=no_think_response_processor,simple_response_processor
# 分组模式（多个 PROMPT_KEY 时）：支持 [proc1,proc2],[proc3],[proc4,proc5] 格式
#   示例：RESPONSE_PROCESSOR=[no_think_response_processor,json_load_response_processor],[simple_response_processor],[no_think_response_processor,simple_response_processor]
#   说明：第1个prompt使用2个处理器，第2个prompt使用1个处理器，第3个prompt使用2个处理器
# 注意：多个 PROMPT_KEY 时，RESPONSE_PROCESSOR 数量必须为 1 个或与 PROMPT_KEY 数量相同（非分组模式）
RESPONSE_PROCESSOR=[no_think_response_processor,test_respoonse_processor],[no_think_response_processor]

# ==============分组模式使用示例==============
# 分组模式配置示例（需要同时配置以下三个参数）：
# PROMPT_KEY=prompt1,prompt2,prompt3
# RESPONSE_PROCESSOR=[proc1,proc2],[proc3],[proc4,proc5]
# OUTPUT_COLUMN=[col1,col2],[col3],[col4,col5]
# 
# 含义：
# - prompt1 生成的响应使用 proc1 和 proc2 处理，结果分别保存到 col1 和 col2
# - prompt2 生成的响应使用 proc3 处理，结果保存到 col3
# - prompt3 生成的响应使用 proc4 和 proc5 处理，结果分别保存到 col4 和 col5